{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ca54060-fbc9-46f0-8b61-791870433647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph 1:\n",
      "രാഹുല്‍ ഈശ്വറിന്റെ ലാപ്ടോപ് പോലീസ് കസ്റ്റഡിയിലെടുത്തു.\n",
      "--------------------------------------------------\n",
      "Paragraph 2:\n",
      "രാഹുല്‍ മാങ്കൂട്ടത്തിലിനെ അനുകൂലിച്ച് വിഡിയോ ചെയ്യുന്നത് നിര്‍ത്തില്ലെന്ന് രാഹുല്‍ ഈശ്വര്‍.\n",
      "--------------------------------------------------\n",
      "Paragraph 3:\n",
      "രാഹുല്‍ മാങ്കൂട്ടത്തിലിനെ കണ്ടെത്താന്‍ പ്രത്യേക അന്വേഷണ സംഘം തിരച്ചില്‍ ശക്തമാക്കി.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# version 3.0\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://malayalam.news18.com/news/gulf/nris-and-indian-tourists-can-now-make-upi-qr-payments-in-uae-gh-rv-677255.html\"\n",
    "\n",
    "# Send an HTTP GET request to fetch the page content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the content of the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all tags with class 'newphtbox-in'\n",
    "    p_tags = soup.find_all([\"p\"])\n",
    "\n",
    "    \n",
    "    # Extract the text from each found tag\n",
    "    paragraphs = [p.get_text() for p in p_tags]\n",
    "\n",
    "    # Print all the paragraphs\n",
    "    for idx, para in enumerate(paragraphs, 1):\n",
    "        print(f\"Paragraph {idx}:\\n{para}\\n{'-'*50}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "#   # Extract the text from the first paragraph if it exists\n",
    "#     if p_tags:\n",
    "#         first_paragraph = p_tags[1].get_text().strip()  # Get the first <p> tag and clean the text\n",
    "#         print(f\"First Paragraph:\\n{first_paragraph}\")\n",
    "#     else:\n",
    "#         print(\"No paragraphs found.\")\n",
    "# else:\n",
    "#     print(f\"Failed to retrieve the page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f404453b-1bc0-4b6d-ba9f-7e6408443a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c76e6-7256-4b47-b4d2-adb3ed81f66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "473e2ab9-525b-49c8-b071-8f82e1699ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print all div tags to debug\n",
    "# div_tags = soup.find_all('div')\n",
    "# for idx, div in enumerate(div_tags, 1):\n",
    "#     print(f\"Div {idx}:\\n{div.prettify()}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7445089a-31c7-4f1c-86eb-952ee8106ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c296f87-3a6a-49f8-9199-ab7011c049ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the page. Status code: 401\n"
     ]
    }
   ],
   "source": [
    "# version 3.1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://www.hindustantimes.com/business/now-you-cannot-use-cred-phonepe-amazon-pay-paytm-for-credit-card-payments-101719999488703.html\"\n",
    "\n",
    "# Define the CSS selector for the container (update this based on your inspection)\n",
    "container_selector =  \"#dataHolder\" # Replace with the correct selector if necessary\n",
    "#['.entry-wraper .entry-main-content','#post-container', '.bodycontent-wrapper',\"#story-detail\", \".abp-story-detail\", \".article-body\",'#main_container',\n",
    "#, '.content-inner', '#the-post','#entryArticle', '#story-wrapper',\".story-elements\",\".entry-content\"]\n",
    "\n",
    "# Send an HTTP GET request to fetch the page content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the content of the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the container element using the CSS selector\n",
    "    container = soup.select_one(container_selector)\n",
    "    \n",
    "    if container:\n",
    "        # Now, find all <p> tags within the container\n",
    "        p_tags = container.find_all([\"p\"])\n",
    "        \n",
    "        # Extract the text from each found <p> tag\n",
    "        paragraphs = [p.get_text().strip() for p in p_tags if p.get_text().strip() != \"\"]  # Strip and filter empty paragraphs\n",
    "\n",
    "        # Print all the paragraphs\n",
    "        if paragraphs:\n",
    "            for idx, para in enumerate(paragraphs, 1):\n",
    "                print(f\"Paragraph {idx}:\\n{para}\\n{'-'*50}\")\n",
    "        else:\n",
    "            print(\"No paragraphs found inside the container.\")\n",
    "    else:\n",
    "        print(f\"No container found with selector: {container_selector}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7622b98c-4930-4124-b359-8d5b2ed6f014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890db6b1-28fc-44ca-bd95-75ccffba1647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "160f3bb2-0a55-4d27-810a-059a0800e6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'phtdesc' containers found. Trying alternative search...\n",
      "No description containers found after multiple attempts.\n"
     ]
    }
   ],
   "source": [
    "# version 3.2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://malayalam.news18.com/news/gulf/nris-and-indian-tourists-can-now-make-upi-qr-payments-in-uae-gh-rv-677255.html\"\n",
    "\n",
    "# Send an HTTP GET request to fetch the page content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the content of the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 1. Try to find all the description containers using different classes or tags\n",
    "    # You can broaden the search criteria by looking for multiple classes or tag types\n",
    "    phtdesc_elements = soup.find_all('div', class_='article-body')\n",
    "\n",
    "    if not phtdesc_elements:\n",
    "        # If nothing is found with the class 'phtdesc', try another tag or class\n",
    "        print(\"No 'phtdesc' containers found. Trying alternative search...\")\n",
    "        phtdesc_elements = soup.find_all('div', class_='article-body')  # Adjust this based on inspection\n",
    "\n",
    "    if phtdesc_elements:\n",
    "        print(f\"Extracted {len(phtdesc_elements)} description containers:\\n\")\n",
    "        # Loop through each of the elements and extract the text\n",
    "        for idx, phtdesc in enumerate(phtdesc_elements, 1):\n",
    "            text = phtdesc.get_text().strip()\n",
    "            if text:  # Only print non-empty descriptions\n",
    "                print(f\"Description {idx}:\\n{text}\\n{'-'*50}\")\n",
    "            else:\n",
    "                print(f\"Description {idx} is empty.\\n{'-'*50}\")\n",
    "    else:\n",
    "        print(\"No description containers found after multiple attempts.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80851457-d689-41a0-aad6-c0a035dbd23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph 1:\n",
      "The change has been introduced as these banking institutions are not integrated with the Bharat Bill Payment System (BBPS) platform.\n",
      "--------------------------------------------------\n",
      "Paragraph 2:\n",
      "The Reserve Bank of India (RBI) tweaked the repayment of credit card outstanding amount and the changes are effective from July 1. The central bank said that all credit card bill payments via third-party applications must be routed through the Bharat Bill Payment System (BBPS)- managed by the National Payments Corporation of India (NPCI).\n",
      "--------------------------------------------------\n",
      "Paragraph 3:\n",
      "Read more: Why this tech boss thanked Sam Altman after building Indian LLM under $5 million: ‘For daring us to dream’\n",
      "--------------------------------------------------\n",
      "Paragraph 4:\n",
      "The change has been introduced as these banking institutions are not integrated with the Bharat Bill Payment System (BBPS) platform.\n",
      "--------------------------------------------------\n",
      "Paragraph 5:\n",
      "What is BBPS platform?\n",
      "--------------------------------------------------\n",
      "Paragraph 6:\n",
      "BBPS was developed by the RBI aiming to provide advanced technological solutions to streamline payment collection process for businesses and customers. BBPS enables customers to conveniently make payments through a network of physical outlets like bank branches and collection stores as well as through various digital channels such as apps and websites. This system ensures prompt settlements and accommodates a variety of payment methods for enhanced flexibility and convenience.\n",
      "--------------------------------------------------\n",
      "Paragraph 7:\n",
      "Read more: Koo app shuts down: Read founder Aprameya Radhakrishna’s full ‘final goodbye’ LinkedIn post\n",
      "--------------------------------------------------\n",
      "Paragraph 8:\n",
      "Which banks are live on BBPS as of July 1, 2024?\n",
      "--------------------------------------------------\n",
      "Paragraph 9:\n",
      "The banks which are live on BBPS are: SBI, Kotak Bank, IndusInd Bank, IDBI Bank, AU Small Finance, Canara Bank, Bank Of Baroda, Federal Bank, ICICI Bank, Union Bank, Punjab National Bank and Saraswat Bank.\n",
      "--------------------------------------------------\n",
      "Paragraph 10:\n",
      "Read more: ITR filing for FY2023-24: Authorised banks for tax payments available at e-Pay Tax service\n",
      "--------------------------------------------------\n",
      "Paragraph 11:\n",
      "Which banks are working on integration?\n",
      "--------------------------------------------------\n",
      "Paragraph 12:\n",
      "The banks which are still working on integration to BBPS are: Axis Bank, HDFC Bank, IDFC First Bank, Indian Bank, Indian Overseas Bank and YES Bank.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# version 3.3\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://www.hindustantimes.com/business/now-you-cannot-use-cred-phonepe-amazon-pay-paytm-for-credit-card-payments-101719999488703.html\"\n",
    "\n",
    "# Define headers with a valid User-Agent string\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Send an HTTP GET request to fetch the page content with the headers\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the content of the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Define the CSS selector for the container (update this based on your inspection)\n",
    "    container_selector = \"#dataHolder\"\n",
    "    #[\".ck-content\", #wrapper]\n",
    "    # Find the container element using the CSS selector\n",
    "    container = soup.select_one(container_selector)\n",
    "    \n",
    "    if container:\n",
    "        # Find all <blockquote> tags within the container\n",
    "        \n",
    "        blockquote_tags = container.select(\"p,h2\")\n",
    "\n",
    "        #[\"blockquote\", \"p\",\"strong\",\"li\", \"p,h2\"]\n",
    "        # Extract the text from each found <blockquote> tag\n",
    "        paragraphs = [blockquote.get_text().strip() for blockquote in blockquote_tags if blockquote.get_text().strip() != \"\"]\n",
    "\n",
    "        # Remove the first paragraph if it contains the full article\n",
    "        if paragraphs:\n",
    "            # Assume the first paragraph contains the full article\n",
    "            first_paragraph = paragraphs[0]\n",
    "            # Remove the first paragraph if it seems too long or contains the full article text\n",
    "            if len(first_paragraph.split()) > 100:  # Adjust this threshold if needed\n",
    "                paragraphs = paragraphs[1:]  # Remove the first element\n",
    "\n",
    "            # Print the remaining paragraphs\n",
    "            for idx, para in enumerate(paragraphs, 1):\n",
    "                print(f\"Paragraph {idx}:\\n{para}\\n{'-'*50}\")\n",
    "        else:\n",
    "            print(\"No blockquotes found inside the container.\")\n",
    "    else:\n",
    "        print(f\"No container found with selector: {container_selector}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f35a345b-4807-4be0-a36a-bf91ecbb01f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No container found with selector: .storyDetails\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677466a4-40ed-40f4-a341-39a0f32a9daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f24f434-b5c2-4dab-beca-9fef04996050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 10 <p> tags:\n",
      "\n",
      "Paragraph 1:\n",
      "டெல்லி: யுபிஐ முறையில் பணம் அனுப்பும் போது எதிர்பாராத விதமாகத் தவறான நபருக்குப் பணம் அனுப்பிவிட்டால் என்ன செய்வது என்ற கேள்வி பலருக்கும் வரும்.. ஆனால் கவலை வேண்டாம். இதுபோல தவறான நபருக்குப் பணம் அனுப்பினால் என்ன செய்ய வேண்டும்.. பணத்தைத் திரும்பப் பெற என்ன வழிமுறை இருக்கிறது என்பது குறித்துப் பார்க்கலாம்.\n",
      "--------------------------------------------------\n",
      "Paragraph 2:\n",
      "இந்த டிஜிட்டல் காலத்தில் பொதுமக்கள் பலரும் யுபிஐ மூலமாகவே பணத்தை அனுப்புகிறார்கள். யுபிஐ என்றால் என்ன எனக் குழம்ப வேண்டாம்.. ஜிபே, போன்பே ஆகியவை இந்த யுபிஐ தொழில்நுட்பத்தின் அடிப்படையில் தான் இயங்குகிறது.\n",
      "--------------------------------------------------\n",
      "Paragraph 3:\n",
      "கார்டுகளில் பணம் செலுத்தினால் குறிப்பிட்ட தொகையை கமிஷனாக செலுத்த வேண்டும். ஆனால், யுபிஐயில் வாடிக்கையாளர் மட்டுமின்றி கடைக்காரரும் எந்தவொரு கமிஷனும் செலுத்தத் தேவையில்லை. இதனால் பலரும் இந்த யுபிஐயை பயன்படுத்தத் தொடங்கிவிட்டனர்.\n",
      "--------------------------------------------------\n",
      "Paragraph 4:\n",
      "யுபிஐ இருந்தால் போதும்: இதை யூஸ் செய்ய இணைய வசதியுடன் கூடிய ஒரு ஸ்மார்ட் போன் இருந்தால் போதும். எங்கு வேண்டுமானாலும் எப்போது வேண்டுமானாலும் நேரடியாக ஒருவரது வங்கிக் கணக்கிற்குப் பணம் அனுப்பலாம்.. கமிஷன் இல்லை, நொடிகளில் பணம் அனுப்ப முடிகிறது, சில்லறை பிரச்சினை எனப் பல காரணங்களால் இந்த யுபிஐ முறையைப் பயன்படுத்துவோரின் எண்ணிக்கை தொடர்ந்து அதிகரித்தே வருகிறது.\n",
      "--------------------------------------------------\n",
      "Paragraph 5:\n",
      "\"இனி நிற்காது..\" ராக்கெட்டில் ஏறிடுச்சு தங்கம் விலை.. எவ்வளவு அதிகரிக்கும்?.ஆனந்த் சீனிவாசன் நறுக் மேலும், இதன் கீழ் ஒருவர் மொபைல் எண் மூலமாகவும் பணம் அனுப்பலாம்.. அல்லது QR கோட் மூலமாகவும் பணத்தை அனுப்பலாம்.. QR கோட் பணம் அனுப்பினால் எந்த பிரச்சினையும் வராது. ஸ்கேன் செய்தாலே போதும் யாருக்குப் பணம் அனுப்புகிறோம் என்பது குறித்த தகவல்கள் வரும். அதை செக் செய்துவிட்டு பணத்தை அனுப்பிவிடலாம். அதேநேரம் நம்பர் மூலம் பணம் அனுப்பும் போது அது வேறு யாருக்காவது சென்றுவிட வாய்ப்புகள் உள்ளன. ரிவர்ஸ் செய்ய முடியாது: மற்ற முறைகளைப் போலவே யுபிஐ முறையில் நீங்கள் பணத்தை அனுப்பிவிட்டால் அதை ரிவர்ஸ் செய்ய முடியாது. இதனால் தவறான நபருக்குப் பணம் அனுப்பிவிட்டால் என்ன செய்ய வேண்டும் என்பது பலருக்கும் கேள்வியாகவே இருக்கிறது. ஆனால், கவலை வேண்டாம்.. தவறான நபருக்குப் பணம் அனுப்பினாலும் அதைத் திரும்பப் பெற சில வழிகள் இருக்கிறது. அது குறித்து நாம் பார்க்கலாம். என்ன செய்ய வேண்டும்: முதலில் யாருக்குத் தவறாகப் பணம் அனுப்பிவிட்டீர்களோ.. அவர்களிடம் நேரடியாகத் தொடர்பு கொண்டு கேட்கலாம். பெரும்பாலான செயலிகளில் சாட் ஆப்ஷன் இருக்கிறது. அதை வைத்து நீங்கள் பணம் கேட்கலாம். பெரும்பாலானோர் பணத்தைக் கொடுத்துவிடுவார்கள். இருப்பினும், சிலர் பணத்தைத் தராமல் இழுத்தடிப்பார்கள். அதுபோன்ற நேரங்களில் நீங்கள் செயலியில் உள்ள சப்போர்ட் செக்ஷனை தொடர்பு கொள்ளலாம்.\n",
      "--------------------------------------------------\n",
      "Paragraph 6:\n",
      "அதாவது நீங்கள் கூகுள் பே பயன்படுத்துகிறீர்கள் என்றால் அதிலேயே சப்போர்ட் செக்ஷன் இருக்கும். அதில் என்ன நடந்தது என்பதை விளக்கி, தவறான எண்ணுக்குத் தற்செயலாகப் பணம் அனுப்பிவிட்டதை விளக்கி கம்ப்ளைன்ட் தரலாம். கூகுள்பே மட்டுமின்றி, போன்பே, பேடிஎம் உள்ளிட்ட அனைத்து யுபிஐ செயலிகளிலும் இதை நடைமுறை தான். உங்கள் பிரச்சினையை விளக்கி, கோரிக்கை விடுக்கலாம்.\n",
      "--------------------------------------------------\n",
      "Paragraph 7:\n",
      "போர்டலில் புகார்: இருப்பினும், இதில் உங்களுக்குத் தீர்வு கிடைக்கவில்லை என்றால் நீங்கள் நேரடியாக NPCI போர்ட்டலிலும் புகார் அளிக்கலாம். இதற்காக நீங்கள் NPCI அதிகாரப்பூர்வ தளத்திற்குச் செல்ல வேண்டும். அதில் What we do Tab' என்பதை கிளிக் செய்து அதில் UPI என்பதை கிளிக் செய்யுங்கள்.\n",
      "--------------------------------------------------\n",
      "Paragraph 8:\n",
      "அதில் Dispute Redressal Mechanism கீழ் பரிவர்த்தனை ஐடி, விர்ச்சுவல் பேமெண்ட் முகவரி, மாற்றப்பட்ட தொகை, பரிவர்த்தனை தேதி, மின்னஞ்சல் ஐடி மற்றும் மொபைல் எண் உட்பட அனைத்து பரிவர்த்தனை விவரங்களையும் ஃபில் செய்யவும். புகாருக்கான காரணம் என்பதில் Incorrectly transferred to another account என்பதைத் தேர்வு செய்து புகாரளிக்கவும்.\n",
      "--------------------------------------------------\n",
      "Paragraph 9:\n",
      "வங்கியிடம் புகார்: அதன் பின்னரும் புகார் தீர்க்கப்படவில்லை என்றால் வங்கியிடம் நேரடியாகப் புகார் அளிக்கலாம். பெரும்பாலான புகார்களுக்கு இந்த மேலே சொன்ன முறைகளிலேயே தீர்வு கிடைத்துவிடும். ஒரு வேளை உங்களுக்கு இது எதிலும் தீர்வு கிடைக்கவில்லை என்றால் நீங்கள் அடுத்து வங்கி குறைதீர்ப்பாளர் அல்லது டிஜிட்டல் புகார்களுக்கான குறைதீர்ப்பாளரை தொடர்பு கொள்ள வேண்டும். 30 நாட்களுக்கு பிறகே இவரைத் தொடர்பு கொள்ள வேண்டும்..\n",
      "--------------------------------------------------\n",
      "Paragraph 10:\n",
      "இதைச் செய்தால் போதும்: சும்மா ஒரு வெற்று பேப்பரில் உங்களது புகாரை நீங்கள் எழுதி அளிக்கலாம். ஒம்புட்ஸ்மேன் அலுவலகத்திற்குத் தபால்/ பார்சல் அல்லது நேரில் சென்று கூட புகாரளிக்கலாம். டிஜிட்டல் பரிவர்த்தனைகளுக்கு மின்னஞ்சல் மூலமாகவும் புகாரை அனுப்பலாம். இதற்கு முன்பு புகார் அளித்தும் நடவடிக்கை இல்லை என்பதை விளக்கி, அதற்கான ஆதாரத்தையும் சமர்ப்பிக்க வேண்டும். இதைச் செய்தால் போதும் உங்கள் பணம் உங்களுக்கே வந்துவிடும்.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#version 3.4\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://tamil.oneindia.com/news/delhi/how-to-get-back-money-if-sent-to-wrong-upi-id-621715.html\"\n",
    "\n",
    "# Setup the path to ChromeDriver\n",
    "chrome_driver_path = r'C:\\WebDrivers\\chromedriver-win64\\chromedriver.exe'  # Update the path\n",
    "\n",
    "# Create a Service object\n",
    "service = Service(chrome_driver_path)\n",
    "\n",
    "# Initialize the WebDriver using the Service object\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Open the webpage using Selenium\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load and the specific article content to be present\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.CLASS_NAME, 'oi-article-lt'))\n",
    ")\n",
    "\n",
    "# Get the page source after the page is loaded\n",
    "html_content = driver.page_source\n",
    "\n",
    "# Use BeautifulSoup to parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the container div with class 'oi-article-lt'\n",
    "container = soup.find('div', class_='oi-article-lt')\n",
    "\n",
    "# Find all <p> tags within that container\n",
    "p_tags = container.find_all('p') if container else []\n",
    "\n",
    "if p_tags:\n",
    "    print(f\"Extracted {len(p_tags)} <p> tags:\\n\")\n",
    "    for idx, p_tag in enumerate(p_tags, 1):\n",
    "        # Extract the text and join it properly by removing newlines and extra spaces\n",
    "        text = p_tag.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        # Clean up extra whitespace using regex\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "        \n",
    "        # Print the cleaned text\n",
    "        if cleaned_text:\n",
    "            print(f\"Paragraph {idx}:\\n{cleaned_text}\\n{'-'*50}\")\n",
    "else:\n",
    "    print(\"No <p> tags found within the container.\")\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "798ab8e9-7616-43d1-b6a4-5342fd53155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# url = \"https://malayalam.news18.com/news/gulf/nris-and-indian-tourists-can-now-make-upi-qr-payments-in-uae-gh-rv-677255.html\"\n",
    "# headers = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "#                   'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "#                   'Chrome/115.0.0.0 Safari/537.36'\n",
    "# }\n",
    "\n",
    "# response = requests.get(url, headers=headers)\n",
    "# if response.status_code != 200:\n",
    "#     print(\"Failed to retrieve the page. Status:\", response.status_code)\n",
    "#     exit()\n",
    "\n",
    "# soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# # For this News18 Malayalam page, inspect and use correct class name\n",
    "# # I observed that the article content lives under <div class=\"article_body\">\n",
    "# container = soup.find(\"div\", class_=\"article_body\")\n",
    "# if not container:\n",
    "#     print(\"❌ Could not find container with class 'article_body'\")\n",
    "#     print(\"Available div classes around content:\")\n",
    "#     for div in soup.find_all(\"div\")[:20]:\n",
    "#         print(\" \", div.get(\"class\"))\n",
    "#     exit()\n",
    "\n",
    "# # Now get paragraphs (<p>), headings (<h3>), ignoring other junk\n",
    "# blocks = container.find_all([\"p\", \"h3\"])\n",
    "# content = [blk.get_text().strip() for blk in blocks if blk.get_text(strip=True)]\n",
    "\n",
    "# if not content:\n",
    "#     print(\"⚠️ Found the container but no valid paragraphs/headers extracted.\")\n",
    "# else:\n",
    "#     for i, para in enumerate(content, 1):\n",
    "#         print(f\"Paragraph {i}:\\n{para}\\n{'-'*50}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90afcdea-55e7-47e2-b260-272cc9e49e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
